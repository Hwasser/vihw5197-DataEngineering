{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5beb1241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/25 12:18:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/25 12:18:10 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start notebook\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.119:7077\") \\\n",
    "        .appName(\"victor_hwasser_applicationA\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075d8bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both text documents have the same amount of lines: 1862234\n",
      "Number of partitions: 3 and 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A1\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "#conf = spark_session.conf\n",
    "sc = spark_session.sparkContext \n",
    "\n",
    "text_sv = sc.textFile('hdfs://192.168.2.119:9000/europarl/europarl-v7.sv-en.sv')\n",
    "text_en = sc.textFile('hdfs://192.168.2.119:9000/europarl/europarl-v7.sv-en.en')\n",
    "\n",
    "# Uncomment to inspect the data!\n",
    "#text_sv.take(10)\n",
    "\n",
    "count_sv = text_sv.count()\n",
    "count_en = text_en.count()\n",
    "count_sv_par = text_sv.getNumPartitions()\n",
    "count_en_par = text_en.getNumPartitions()\n",
    "\n",
    "if count_sv == count_en:\n",
    "    print(\"Both text documents have the same amount of lines:\", count_sv)\n",
    "    print(\"Number of partitions:\", count_sv_par, \"and\", count_en_par)\n",
    "else:\n",
    "    print(\"The documents doesn't have the same amount of lines, something's wrong!\", count_sv, \"vs\", count_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf9d8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['återupptagande', 'av', 'sessionen'], ['jag', 'förklarar', 'europaparlamentets', 'session', 'återupptagen', 'efter', 'avbrottet', 'den', '17', 'december.', 'jag', 'vill', 'på', 'nytt', 'önska', 'er', 'ett', 'gott', 'nytt', 'år', 'och', 'jag', 'hoppas', 'att', 'ni', 'haft', 'en', 'trevlig', 'semester.'], ['som', 'ni', 'kunnat', 'konstatera', 'ägde', '\"den', 'stora', 'år', '2000-buggen\"', 'aldrig', 'rum.', 'däremot', 'har', 'invånarna', 'i', 'ett', 'antal', 'av', 'våra', 'medlemsländer', 'drabbats', 'av', 'naturkatastrofer', 'som', 'verkligen', 'varit', 'förskräckliga.'], ['ni', 'har', 'begärt', 'en', 'debatt', 'i', 'ämnet', 'under', 'sammanträdesperiodens', 'kommande', 'dagar.'], ['till', 'dess', 'vill', 'jag', 'att', 'vi,', 'som', 'ett', 'antal', 'kolleger', 'begärt,', 'håller', 'en', 'tyst', 'minut', 'för', 'offren', 'för', 'bl.a.', 'stormarna', 'i', 'de', 'länder', 'i', 'europeiska', 'unionen', 'som', 'drabbats.'], ['jag', 'ber', 'er', 'resa', 'er', 'för', 'en', 'tyst', 'minut.'], ['(parlamentet', 'höll', 'en', 'tyst', 'minut.)'], ['fru', 'talman!', 'det', 'gäller', 'en', 'ordningsfråga.'], ['ni', 'känner', 'till', 'från', 'media', 'att', 'det', 'skett', 'en', 'rad', 'bombexplosioner', 'och', 'mord', 'i', 'sri', 'lanka.'], ['en', 'av', 'de', 'personer', 'som', 'mycket', 'nyligen', 'mördades', 'i', 'sri', 'lanka', 'var', 'kumar', 'ponnambalam,', 'som', 'besökte', 'europaparlamentet', 'för', 'bara', 'några', 'månader', 'sedan.']]\n",
      "[['resumption', 'of', 'the', 'session'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.'], ['although,', 'as', 'you', 'will', 'have', 'seen,', 'the', 'dreaded', \"'millennium\", \"bug'\", 'failed', 'to', 'materialise,', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful.'], ['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days,', 'during', 'this', 'part-session.'], ['in', 'the', 'meantime,', 'i', 'should', 'like', 'to', 'observe', 'a', \"minute'\", 's', 'silence,', 'as', 'a', 'number', 'of', 'members', 'have', 'requested,', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned,', 'particularly', 'those', 'of', 'the', 'terrible', 'storms,', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union.'], ['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.'], ['(the', 'house', 'rose', 'and', 'observed', 'a', \"minute'\", 's', 'silence)'], ['madam', 'president,', 'on', 'a', 'point', 'of', 'order.'], ['you', 'will', 'be', 'aware', 'from', 'the', 'press', 'and', 'television', 'that', 'there', 'have', 'been', 'a', 'number', 'of', 'bomb', 'explosions', 'and', 'killings', 'in', 'sri', 'lanka.'], ['one', 'of', 'the', 'people', 'assassinated', 'very', 'recently', 'in', 'sri', 'lanka', 'was', 'mr', 'kumar', 'ponnambalam,', 'who', 'had', 'visited', 'the', 'european', 'parliament', 'just', 'a', 'few', 'months', 'ago.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both text documents have the same amount of lines: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:=============================>                             (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A2\n",
    "\n",
    "def pre_text(text):\n",
    "    # Make each sentence lower case\n",
    "    text_lower = text.map(lambda x: x.lower())\n",
    "    # split each sentence into words\n",
    "    #!! It would make more sence to use flatMap, but this wouldn't word for A 2.3\n",
    "    words = text_lower.map(lambda x: x.split())\n",
    "    return words\n",
    "\n",
    "# get rdd with processed text\n",
    "words_sv = pre_text(text_sv)\n",
    "words_en = pre_text(text_en)\n",
    "\n",
    "# A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing.\n",
    "first_sv = words_sv.take(10)\n",
    "first_en = words_en.take(10)\n",
    "\n",
    "print(first_sv)\n",
    "print(first_en)\n",
    "\n",
    "# A.2.3 Verify that the line counts still match after the pre-processing.\n",
    "count_sv = words_sv.count()\n",
    "count_en = words_en.count()\n",
    "\n",
    "if count_sv == count_en:\n",
    "    print(\"Both text documents have the same amount of lines:\", count_sv)\n",
    "else:\n",
    "    print(\"The documents doesn't have the same amount of lines, something's wrong!\", count_sv, \"vs\", count_en)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f325593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3498574), ('of', 1659884), ('to', 1539823), ('and', 1288620), ('in', 1086089), ('that', 797576), ('a', 773812), ('is', 758087), ('for', 534270), ('we', 522879)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('att', 1706309), ('och', 1344895), ('i', 1050989), ('det', 924878), ('som', 913302), ('för', 908703), ('av', 738102), ('är', 694389), ('en', 620347), ('vi', 539808)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A3\n",
    "\n",
    "# word count, basically a map-reducer\n",
    "def count_words(text):\n",
    "    # Make each sentence lower case\n",
    "    text_lower = text.map(lambda x: x.lower())\n",
    "    # split each sentence into words and flatten\n",
    "    words = text_lower.flatMap(lambda x: x.split())\n",
    "    # reduce words\n",
    "    counted_words = words.map(lambda x: (x, 1)).reduceByKey(lambda x,y: x + y)\n",
    "    return counted_words\n",
    "\n",
    "# The algorithm here is to do a MapReduce, sort the values and take the top ten ones\n",
    "counted_words_en = count_words(text_en)\n",
    "sorted_words_en = counted_words_en.sortBy(lambda x: x[1], False)\n",
    "top_en = sorted_words_en.take(10)\n",
    "print(top_en)\n",
    "\n",
    "# Doing the same thing to swedish words\n",
    "counted_words_sv = count_words(text_sv)\n",
    "sorted_words_sv = counted_words_sv.sortBy(lambda x: x[1], False)\n",
    "top_sv = sorted_words_sv.take(10)\n",
    "print(top_sv)\n",
    "\n",
    "# A.3.2 Verify that your results are reasonable.\n",
    "# The most common words are:\n",
    "# En: the, of, to, and, in, that, a\n",
    "# Se: att, och, i, det, som, för, av\n",
    "# This makes sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bbf5ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('toppmötet', 'eu-russia'), 3), (('det', 'that'), 291), (('inte.', 'not.'), 51), (('mig.', 'me.'), 9), (('obegripligt!', 'incomprehensible!'), 2), (('nödvändigt.', 'necessary.'), 3), (('är', 'is'), 792), (('tack,', 'thank'), 264), (('kommissionär!', 'commissioner.'), 36), (('stor?', 'big?'), 1)]\n",
      "[(('(applåder)', '(applause)'), 2546), (('.', '.'), 2081), (('är', 'is'), 792), (('applåder', 'applause'), 451), (('1.', '1.'), 438), (('2.', '2.'), 438), (('3.', '3.'), 405), (('varför?', 'why?'), 369), (('det', 'that'), 291), (('tack,', 'thank'), 264)]\n"
     ]
    }
   ],
   "source": [
    "# A4\n",
    "\n",
    "sv_1 = words_sv.zipWithIndex()\n",
    "en_1 = words_en.zipWithIndex()\n",
    "sv_2 = sv_1.map(lambda x: (x[1],x[0]))\n",
    "en_2 = en_1.map(lambda x: (x[1],x[0]))\n",
    "step3 = sv_2.join(en_2)\n",
    "step4 = step3.filter(lambda x: x[1][0] != [] and x[1][1] != [])\n",
    "step5 = step4.filter(lambda x: len(x[1][0]) < 4)\n",
    "step6 = step5.filter(lambda x: len(x[1][0]) == len(x[1][1]))\n",
    "step7 = step6.flatMap(lambda x: list(zip(x[1][0], x[1][1])))\n",
    "step8 = step7.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "print(step8.take(10))\n",
    "step9 = step8.sortBy(lambda x: x[1], False)\n",
    "print(step9.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bb317b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor this section, we’ll use the PySpark DataFrames/SQL API. Use the existing cluster, and a\\nnotebook on your own client machine, which you must deploy yourself.\\nWe’ll work with a large dataset in CSV format. Our dataset is the Los Angeles Parking\\nCitations (https://www.kaggle.com/cityofLA/los-angeles-parking-citations). I have pre-loaded\\nthe dataset into the HDFS cluster.\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For this section, we’ll use the PySpark DataFrames/SQL API. Use the existing cluster, and a\n",
    "notebook on your own client machine, which you must deploy yourself.\n",
    "We’ll work with a large dataset in CSV format. Our dataset is the Los Angeles Parking\n",
    "Citations (https://www.kaggle.com/cityofLA/los-angeles-parking-citations). I have pre-loaded\n",
    "the dataset into the HDFS cluster.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd51a115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+--------------------+-----------+---------+---------+------------------+-----------------+--------------------+\n",
      "|          _c0|                 _c1|       _c2|     _c3|        _c4|           _c5|              _c6| _c7| _c8|       _c9| _c10|                _c11| _c12|  _c13|          _c14|                _c15|       _c16|     _c17|     _c18|              _c19|             _c20|                _c21|\n",
      "+-------------+--------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+--------------------+-----------+---------+---------+------------------+-----------------+--------------------+\n",
      "|Ticket number|          Issue Date|Issue time|Meter Id|Marked Time|RP State Plate|Plate Expiry Date| VIN|Make|Body Style|Color|            Location|Route|Agency|Violation code|Violation Descrip...|Fine amount| Latitude|Longitude|Agency Description|Color Description|Body Style Descri...|\n",
      "|   1103341116|2015-12-21T00:00:...|      1251|    null|       null|            CA|           200304|null|HOND|        PA|   GY|     13147 WELBY WAY|01521|     1|        4000A1|  NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                null|\n",
      "|   1103700150|2015-12-21T00:00:...|      1435|    null|       null|            CA|           201512|null| GMC|        VN|   WH|       525 S MAIN ST| 1C51|     1|        4000A1|  NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                null|\n",
      "|   1104803000|2015-12-21T00:00:...|      2055|    null|       null|            CA|           201503|null|NISS|        PA|   BK|       200 WORLD WAY|  2R2|     2|          8939|          WHITE CURB|         58|6439997.9|1802686.4|              null|             null|                null|\n",
      "|   1104820732|2015-12-26T00:00:...|      1515|    null|       null|            CA|             null|null|ACUR|        PA|   WH|       100 WORLD WAY| 2F11|     2|           000|              17104h|       null|6440041.1|1802686.2|              null|             null|                null|\n",
      "|   1105461453|2015-09-15T00:00:...|       115|    null|       null|            CA|           200316|null|CHEV|        PA|   BK|  GEORGIA ST/OLYMPIC|1FB70|     1|         8069A|NO STOPPING/STANDING|         93|    99999|    99999|              null|             null|                null|\n",
      "|   1106226590|2015-09-15T00:00:...|        19|    null|       null|            CA|           201507|null|CHEV|        VN|   GY|  SAN PEDRO S/O BOYD|1A35W|     1|        4000A1|  NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                null|\n",
      "|   1106500452|2015-12-17T00:00:...|      1710|    null|       null|            CA|           201605|null|MAZD|        PA|   BL|     SUNSET/ALVARADO|00217|     1|          8070|PARK IN GRID LOCK ZN|        163|    99999|    99999|              null|             null|                null|\n",
      "|   1106500463|2015-12-17T00:00:...|      1710|    null|       null|            CA|           201602|null|TOYO|        PA|   BK|     SUNSET/ALVARADO|00217|     1|          8070|PARK IN GRID LOCK ZN|        163|    99999|    99999|              null|             null|                null|\n",
      "|   1106506402|2015-12-22T00:00:...|       945|    null|       null|            CA|           201605|null|CHEV|        PA|   BR|      721 S WESTLAKE| 2A75|     1|        8069AA|    NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                null|\n",
      "|   1106506413|2015-12-22T00:00:...|      1100|    null|       null|            CA|           201701|null|NISS|        PA|   SI|     1159 HUNTLEY DR| 2A75|     1|        8069AA|    NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                null|\n",
      "|   1106506424|2015-12-22T00:00:...|      1100|    null|       null|            CA|           201511|null|FORD|        TR|   WH|     1159 HUNTLEY DR| 2A75|     1|        8069AA|    NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                null|\n",
      "|   1106506435|2015-12-22T00:00:...|      1105|    null|       null|            CA|           201701|null|CHRY|        PA|   GO|     1159 HUNTLEY DR| 2A75|     1|        8069AA|    NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                null|\n",
      "|   1106506446|2015-12-22T00:00:...|      1110|    null|       null|            CA|           201511|null| BMW|        PA|   BK|      1200 W MIRAMAR| 2A75|     1|        4000A1|  NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                null|\n",
      "|   1106549754|2015-12-15T00:00:...|       825|    null|       null|            CA|           201607|null|PTRB|        TR|   BK|           4TH/STATE| CM96|     1|         8069A|NO STOPPING/STANDING|         93|    99999|    99999|              null|             null|                null|\n",
      "|   1107179581|2015-12-27T00:00:...|      1055|    null|       null|            CA|           201605|null|TOYO|        PA|   BK|3100 N HOLLYRIDGE DR| null|    54|         8058L|        PREF PARKING|         68|    99999|    99999|              null|             null|                null|\n",
      "|   1107179592|2015-12-27T00:00:...|      1200|    null|       null|            CA|           201602|null|MBNZ|        PA|   BK|   3115 N BERENDO DR| null|    54|         8058L|        PREF PARKING|         68|    99999|    99999|              null|             null|                null|\n",
      "|   1107179603|2015-12-27T00:00:...|      1400|    null|       null|            CA|           201611|null|NISS|        PA|   WH| 3100 N BEACHWOOD DR| null|    54|         8058L|        PREF PARKING|         68|    99999|    99999|              null|             null|                null|\n",
      "|   1107539823|2015-09-16T00:00:...|      2120|    null|       null|            CA|           201502|null|NISS|        PA| null|      BLAINE/11TH PL|1FB95|     1|        4000A1|  NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                null|\n",
      "|   1107539834|2015-09-16T00:00:...|      1045|    null|       null|            CA|             null|null|CHEV|        PA|   BK|  1246 S FIGUEROA ST| 1L20|     1|        8069AP|    NO STOP/STAND PM|         93|    99999|    99999|              null|             null|                null|\n",
      "+-------------+--------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+--------------------+-----------+---------+---------+------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B.1 Load the CSV file from HDFS, and call show() to verify the data is loaded correctly.\n",
    "df = spark_session.read.csv('hdfs://192.168.2.119:9000/parking-citations.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baad622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
