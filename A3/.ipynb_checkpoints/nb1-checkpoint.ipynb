{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5beb1241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/25 13:38:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/25 13:38:24 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start notebook\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.119:7077\") \\\n",
    "        .appName(\"victor_hwasser_applicationA\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075d8bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both text documents have the same amount of lines: 1862234\n",
      "Number of partitions: 3 and 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A1\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "#conf = spark_session.conf\n",
    "sc = spark_session.sparkContext \n",
    "\n",
    "text_sv = sc.textFile('hdfs://192.168.2.119:9000/europarl/europarl-v7.sv-en.sv')\n",
    "text_en = sc.textFile('hdfs://192.168.2.119:9000/europarl/europarl-v7.sv-en.en')\n",
    "\n",
    "# Uncomment to inspect the data!\n",
    "#text_sv.take(10)\n",
    "\n",
    "count_sv = text_sv.count()\n",
    "count_en = text_en.count()\n",
    "count_sv_par = text_sv.getNumPartitions()\n",
    "count_en_par = text_en.getNumPartitions()\n",
    "\n",
    "if count_sv == count_en:\n",
    "    print(\"Both text documents have the same amount of lines:\", count_sv)\n",
    "    print(\"Number of partitions:\", count_sv_par, \"and\", count_en_par)\n",
    "else:\n",
    "    print(\"The documents doesn't have the same amount of lines, something's wrong!\", count_sv, \"vs\", count_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf9d8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['återupptagande', 'av', 'sessionen'], ['jag', 'förklarar', 'europaparlamentets', 'session', 'återupptagen', 'efter', 'avbrottet', 'den', '17', 'december.', 'jag', 'vill', 'på', 'nytt', 'önska', 'er', 'ett', 'gott', 'nytt', 'år', 'och', 'jag', 'hoppas', 'att', 'ni', 'haft', 'en', 'trevlig', 'semester.'], ['som', 'ni', 'kunnat', 'konstatera', 'ägde', '\"den', 'stora', 'år', '2000-buggen\"', 'aldrig', 'rum.', 'däremot', 'har', 'invånarna', 'i', 'ett', 'antal', 'av', 'våra', 'medlemsländer', 'drabbats', 'av', 'naturkatastrofer', 'som', 'verkligen', 'varit', 'förskräckliga.'], ['ni', 'har', 'begärt', 'en', 'debatt', 'i', 'ämnet', 'under', 'sammanträdesperiodens', 'kommande', 'dagar.'], ['till', 'dess', 'vill', 'jag', 'att', 'vi,', 'som', 'ett', 'antal', 'kolleger', 'begärt,', 'håller', 'en', 'tyst', 'minut', 'för', 'offren', 'för', 'bl.a.', 'stormarna', 'i', 'de', 'länder', 'i', 'europeiska', 'unionen', 'som', 'drabbats.'], ['jag', 'ber', 'er', 'resa', 'er', 'för', 'en', 'tyst', 'minut.'], ['(parlamentet', 'höll', 'en', 'tyst', 'minut.)'], ['fru', 'talman!', 'det', 'gäller', 'en', 'ordningsfråga.'], ['ni', 'känner', 'till', 'från', 'media', 'att', 'det', 'skett', 'en', 'rad', 'bombexplosioner', 'och', 'mord', 'i', 'sri', 'lanka.'], ['en', 'av', 'de', 'personer', 'som', 'mycket', 'nyligen', 'mördades', 'i', 'sri', 'lanka', 'var', 'kumar', 'ponnambalam,', 'som', 'besökte', 'europaparlamentet', 'för', 'bara', 'några', 'månader', 'sedan.']]\n",
      "[['resumption', 'of', 'the', 'session'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.'], ['although,', 'as', 'you', 'will', 'have', 'seen,', 'the', 'dreaded', \"'millennium\", \"bug'\", 'failed', 'to', 'materialise,', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful.'], ['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days,', 'during', 'this', 'part-session.'], ['in', 'the', 'meantime,', 'i', 'should', 'like', 'to', 'observe', 'a', \"minute'\", 's', 'silence,', 'as', 'a', 'number', 'of', 'members', 'have', 'requested,', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned,', 'particularly', 'those', 'of', 'the', 'terrible', 'storms,', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union.'], ['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.'], ['(the', 'house', 'rose', 'and', 'observed', 'a', \"minute'\", 's', 'silence)'], ['madam', 'president,', 'on', 'a', 'point', 'of', 'order.'], ['you', 'will', 'be', 'aware', 'from', 'the', 'press', 'and', 'television', 'that', 'there', 'have', 'been', 'a', 'number', 'of', 'bomb', 'explosions', 'and', 'killings', 'in', 'sri', 'lanka.'], ['one', 'of', 'the', 'people', 'assassinated', 'very', 'recently', 'in', 'sri', 'lanka', 'was', 'mr', 'kumar', 'ponnambalam,', 'who', 'had', 'visited', 'the', 'european', 'parliament', 'just', 'a', 'few', 'months', 'ago.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both text documents have the same amount of lines: 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:=============================>                             (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A2\n",
    "\n",
    "def pre_text(text):\n",
    "    # Make each sentence lower case\n",
    "    text_lower = text.map(lambda x: x.lower())\n",
    "    # split each sentence into words\n",
    "    #!! It would make more sence to use flatMap, but this wouldn't word for A 2.3\n",
    "    words = text_lower.map(lambda x: x.split())\n",
    "    return words\n",
    "\n",
    "# get rdd with processed text\n",
    "words_sv = pre_text(text_sv)\n",
    "words_en = pre_text(text_en)\n",
    "\n",
    "# A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing.\n",
    "first_sv = words_sv.take(10)\n",
    "first_en = words_en.take(10)\n",
    "\n",
    "print(first_sv)\n",
    "print(first_en)\n",
    "\n",
    "# A.2.3 Verify that the line counts still match after the pre-processing.\n",
    "count_sv = words_sv.count()\n",
    "count_en = words_en.count()\n",
    "\n",
    "if count_sv == count_en:\n",
    "    print(\"Both text documents have the same amount of lines:\", count_sv)\n",
    "else:\n",
    "    print(\"The documents doesn't have the same amount of lines, something's wrong!\", count_sv, \"vs\", count_en)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f325593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3498574), ('of', 1659884), ('to', 1539823), ('and', 1288620), ('in', 1086089), ('that', 797576), ('a', 773812), ('is', 758087), ('for', 534270), ('we', 522879)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('att', 1706309), ('och', 1344895), ('i', 1050989), ('det', 924878), ('som', 913302), ('för', 908703), ('av', 738102), ('är', 694389), ('en', 620347), ('vi', 539808)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# A3\n",
    "\n",
    "# word count, basically a map-reducer\n",
    "def count_words(text):\n",
    "    # Make each sentence lower case\n",
    "    text_lower = text.map(lambda x: x.lower())\n",
    "    # split each sentence into words and flatten\n",
    "    words = text_lower.flatMap(lambda x: x.split())\n",
    "    # reduce words\n",
    "    counted_words = words.map(lambda x: (x, 1)).reduceByKey(lambda x,y: x + y)\n",
    "    return counted_words\n",
    "\n",
    "# The algorithm here is to do a MapReduce, sort the values and take the top ten ones\n",
    "counted_words_en = count_words(text_en)\n",
    "sorted_words_en = counted_words_en.sortBy(lambda x: x[1], False)\n",
    "top_en = sorted_words_en.take(10)\n",
    "print(top_en)\n",
    "\n",
    "# Doing the same thing to swedish words\n",
    "counted_words_sv = count_words(text_sv)\n",
    "sorted_words_sv = counted_words_sv.sortBy(lambda x: x[1], False)\n",
    "top_sv = sorted_words_sv.take(10)\n",
    "print(top_sv)\n",
    "\n",
    "# A.3.2 Verify that your results are reasonable.\n",
    "# The most common words are:\n",
    "# En: the, of, to, and, in, that, a\n",
    "# Se: att, och, i, det, som, för, av\n",
    "# This makes sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bbf5ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('toppmötet', 'eu-russia'), 3), (('det', 'that'), 291), (('inte.', 'not.'), 51), (('mig.', 'me.'), 9), (('obegripligt!', 'incomprehensible!'), 2), (('nödvändigt.', 'necessary.'), 3), (('är', 'is'), 792), (('tack,', 'thank'), 264), (('kommissionär!', 'commissioner.'), 36), (('stor?', 'big?'), 1)]\n",
      "[(('(applåder)', '(applause)'), 2546), (('.', '.'), 2081), (('är', 'is'), 792), (('applåder', 'applause'), 451), (('1.', '1.'), 438), (('2.', '2.'), 438), (('3.', '3.'), 405), (('varför?', 'why?'), 369), (('det', 'that'), 291), (('tack,', 'thank'), 264)]\n"
     ]
    }
   ],
   "source": [
    "# A4\n",
    "\n",
    "sv_1 = words_sv.zipWithIndex()\n",
    "en_1 = words_en.zipWithIndex()\n",
    "sv_2 = sv_1.map(lambda x: (x[1],x[0]))\n",
    "en_2 = en_1.map(lambda x: (x[1],x[0]))\n",
    "step3 = sv_2.join(en_2)\n",
    "step4 = step3.filter(lambda x: x[1][0] != [] and x[1][1] != [])\n",
    "step5 = step4.filter(lambda x: len(x[1][0]) < 4)\n",
    "step6 = step5.filter(lambda x: len(x[1][0]) == len(x[1][1]))\n",
    "step7 = step6.flatMap(lambda x: list(zip(x[1][0], x[1][1])))\n",
    "step8 = step7.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "print(step8.take(10))\n",
    "step9 = step8.sortBy(lambda x: x[1], False)\n",
    "print(step9.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bb317b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor this section, we’ll use the PySpark DataFrames/SQL API. Use the existing cluster, and a\\nnotebook on your own client machine, which you must deploy yourself.\\nWe’ll work with a large dataset in CSV format. Our dataset is the Los Angeles Parking\\nCitations (https://www.kaggle.com/cityofLA/los-angeles-parking-citations). I have pre-loaded\\nthe dataset into the HDFS cluster.\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For this section, we’ll use the PySpark DataFrames/SQL API. Use the existing cluster, and a\n",
    "notebook on your own client machine, which you must deploy yourself.\n",
    "We’ll work with a large dataset in CSV format. Our dataset is the Los Angeles Parking\n",
    "Citations (https://www.kaggle.com/cityofLA/los-angeles-parking-citations). I have pre-loaded\n",
    "the dataset into the HDFS cluster.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd51a115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+---------------------+-----------+---------+---------+------------------+-----------------+----------------------+\n",
      "|Ticket number|          Issue Date|Issue time|Meter Id|Marked Time|RP State Plate|Plate Expiry Date| VIN|Make|Body Style|Color|            Location|Route|Agency|Violation code|Violation Description|Fine amount| Latitude|Longitude|Agency Description|Color Description|Body Style Description|\n",
      "+-------------+--------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+---------------------+-----------+---------+---------+------------------+-----------------+----------------------+\n",
      "|   1103341116|2015-12-21T00:00:...|      1251|    null|       null|            CA|           200304|null|HOND|        PA|   GY|     13147 WELBY WAY|01521|     1|        4000A1|   NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                  null|\n",
      "|   1103700150|2015-12-21T00:00:...|      1435|    null|       null|            CA|           201512|null| GMC|        VN|   WH|       525 S MAIN ST| 1C51|     1|        4000A1|   NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                  null|\n",
      "|   1104803000|2015-12-21T00:00:...|      2055|    null|       null|            CA|           201503|null|NISS|        PA|   BK|       200 WORLD WAY|  2R2|     2|          8939|           WHITE CURB|         58|6439997.9|1802686.4|              null|             null|                  null|\n",
      "|   1104820732|2015-12-26T00:00:...|      1515|    null|       null|            CA|             null|null|ACUR|        PA|   WH|       100 WORLD WAY| 2F11|     2|           000|               17104h|       null|6440041.1|1802686.2|              null|             null|                  null|\n",
      "|   1105461453|2015-09-15T00:00:...|       115|    null|       null|            CA|           200316|null|CHEV|        PA|   BK|  GEORGIA ST/OLYMPIC|1FB70|     1|         8069A| NO STOPPING/STANDING|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1106226590|2015-09-15T00:00:...|        19|    null|       null|            CA|           201507|null|CHEV|        VN|   GY|  SAN PEDRO S/O BOYD|1A35W|     1|        4000A1|   NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                  null|\n",
      "|   1106500452|2015-12-17T00:00:...|      1710|    null|       null|            CA|           201605|null|MAZD|        PA|   BL|     SUNSET/ALVARADO|00217|     1|          8070| PARK IN GRID LOCK ZN|        163|    99999|    99999|              null|             null|                  null|\n",
      "|   1106500463|2015-12-17T00:00:...|      1710|    null|       null|            CA|           201602|null|TOYO|        PA|   BK|     SUNSET/ALVARADO|00217|     1|          8070| PARK IN GRID LOCK ZN|        163|    99999|    99999|              null|             null|                  null|\n",
      "|   1106506402|2015-12-22T00:00:...|       945|    null|       null|            CA|           201605|null|CHEV|        PA|   BR|      721 S WESTLAKE| 2A75|     1|        8069AA|     NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1106506413|2015-12-22T00:00:...|      1100|    null|       null|            CA|           201701|null|NISS|        PA|   SI|     1159 HUNTLEY DR| 2A75|     1|        8069AA|     NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1106506424|2015-12-22T00:00:...|      1100|    null|       null|            CA|           201511|null|FORD|        TR|   WH|     1159 HUNTLEY DR| 2A75|     1|        8069AA|     NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1106506435|2015-12-22T00:00:...|      1105|    null|       null|            CA|           201701|null|CHRY|        PA|   GO|     1159 HUNTLEY DR| 2A75|     1|        8069AA|     NO STOP/STAND AM|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1106506446|2015-12-22T00:00:...|      1110|    null|       null|            CA|           201511|null| BMW|        PA|   BK|      1200 W MIRAMAR| 2A75|     1|        4000A1|   NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                  null|\n",
      "|   1106549754|2015-12-15T00:00:...|       825|    null|       null|            CA|           201607|null|PTRB|        TR|   BK|           4TH/STATE| CM96|     1|         8069A| NO STOPPING/STANDING|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1107179581|2015-12-27T00:00:...|      1055|    null|       null|            CA|           201605|null|TOYO|        PA|   BK|3100 N HOLLYRIDGE DR| null|    54|         8058L|         PREF PARKING|         68|    99999|    99999|              null|             null|                  null|\n",
      "|   1107179592|2015-12-27T00:00:...|      1200|    null|       null|            CA|           201602|null|MBNZ|        PA|   BK|   3115 N BERENDO DR| null|    54|         8058L|         PREF PARKING|         68|    99999|    99999|              null|             null|                  null|\n",
      "|   1107179603|2015-12-27T00:00:...|      1400|    null|       null|            CA|           201611|null|NISS|        PA|   WH| 3100 N BEACHWOOD DR| null|    54|         8058L|         PREF PARKING|         68|    99999|    99999|              null|             null|                  null|\n",
      "|   1107539823|2015-09-16T00:00:...|      2120|    null|       null|            CA|           201502|null|NISS|        PA| null|      BLAINE/11TH PL|1FB95|     1|        4000A1|   NO EVIDENCE OF REG|         50|    99999|    99999|              null|             null|                  null|\n",
      "|   1107539834|2015-09-16T00:00:...|      1045|    null|       null|            CA|             null|null|CHEV|        PA|   BK|  1246 S FIGUEROA ST| 1L20|     1|        8069AP|     NO STOP/STAND PM|         93|    99999|    99999|              null|             null|                  null|\n",
      "|   1107780811|2015-12-22T00:00:...|      1102|    null|       null|            CA|           201606|null|HOND|        PA|   BK|       PLATA/RAMPART|  2A1|     1|         8069B|           NO PARKING|         73|    99999|    99999|              null|             null|                  null|\n",
      "+-------------+--------------------+----------+--------+-----------+--------------+-----------------+----+----+----------+-----+--------------------+-----+------+--------------+---------------------+-----------+---------+---------+------------------+-----------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B.1 Load the CSV file from HDFS, and call show() to verify the data is loaded correctly.\n",
    "df = spark_session.read.csv('hdfs://192.168.2.119:9000/parking-citations.csv', header=True)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7baad622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Ticket number,StringType,true),StructField(Issue Date,StringType,true),StructField(Issue time,StringType,true),StructField(Meter Id,StringType,true),StructField(Marked Time,StringType,true),StructField(RP State Plate,StringType,true),StructField(Plate Expiry Date,StringType,true),StructField(VIN,StringType,true),StructField(Make,StringType,true),StructField(Body Style,StringType,true),StructField(Color,StringType,true),StructField(Location,StringType,true),StructField(Route,StringType,true),StructField(Agency,StringType,true),StructField(Violation code,StringType,true),StructField(Violation Description,StringType,true),StructField(Fine amount,StringType,true),StructField(Latitude,StringType,true),StructField(Longitude,StringType,true),StructField(Agency Description,StringType,true),StructField(Color Description,StringType,true),StructField(Body Style Description,StringType,true)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13077724\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# B.2 Print the schema for the DataFrame.\n",
    "\n",
    "print(df.schema)\n",
    "\n",
    "# B.3 Count the number of rows in the CSV file.\n",
    "\n",
    "df_count = df.count()\n",
    "print(df_count)\n",
    "\n",
    "# B.4 Count the number of partitions in the underlying RDD.\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc319b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ticket number', 'Issue Date', 'Issue time', 'Meter Id', 'Marked Time', 'RP State Plate', 'Plate Expiry Date', 'Make', 'Body Style', 'Color', 'Location', 'Route', 'Agency', 'Violation code', 'Violation Description', 'Fine amount', 'Agency Description', 'Color Description', 'Body Style Description']\n"
     ]
    }
   ],
   "source": [
    "# B.5 Drop the columns VIN, Latitude and Longitude.\n",
    "\n",
    "columns_to_drop = ['VIN', 'Latitude', 'Longitude']\n",
    "for c in columns_to_drop:\n",
    "    df = df.drop(c)\n",
    "    \n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d928653",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "all exprs should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# B.6 Find the maximum fine amount. How many fines have this amount? \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# You need to convert the ‘fine amount’ column to a float to do this correctly.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m fine_amount \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFine amount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:1832\u001b[0m, in \u001b[0;36mDataFrame.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mexprs):\n\u001b[1;32m   1819\u001b[0m     \u001b[38;5;124;03m\"\"\" Aggregate on the entire :class:`DataFrame` without groups\u001b[39;00m\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;124;03m    (shorthand for ``df.groupBy().agg()``).\u001b[39;00m\n\u001b[1;32m   1821\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;124;03m    [Row(min(age)=2)]\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexprs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/group.py:117\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jgd\u001b[38;5;241m.\u001b[39magg(exprs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Columns\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jgd\u001b[38;5;241m.\u001b[39magg(exprs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_jc,\n\u001b[1;32m    119\u001b[0m                         _to_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx\u001b[38;5;241m.\u001b[39m_sc, [c\u001b[38;5;241m.\u001b[39m_jc \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs[\u001b[38;5;241m1\u001b[39m:]]))\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "\u001b[0;31mAssertionError\u001b[0m: all exprs should be Column"
     ]
    }
   ],
   "source": [
    "# B.6 Find the maximum fine amount. How many fines have this amount? \n",
    "# You need to convert the ‘fine amount’ column to a float to do this correctly.\n",
    "\n",
    "fine_amount = df.agg(max('Fine amount'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
